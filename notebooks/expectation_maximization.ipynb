{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two techniques are covered in this notebook\n",
    "- Bernoulli (e.g. coin toss)\n",
    "- Gaussian Mixture Model (e.g. cluster of continuous data)\n",
    "\n",
    "There are four steps to expectation maximization:\n",
    "- setting the joint\n",
    "- computing the expectation\n",
    "- expectation step\n",
    "- minization step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli (coin toss example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method**\n",
    "- Pick up one of the coins up at random and flip the coin 10 times and record the results \n",
    "- Repeat this picking up 5 times\n",
    "\n",
    "This will result in 5 groups of 10 flips.\n",
    "\n",
    "We want to find out, for each group, which coin was flipped ($c_{1}$ and $c_{2}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior**\n",
    "\n",
    "We believe that that each coin has a different probability of getting a head:\n",
    "- $\\theta_{A}^{(0)} = 0.60$\n",
    "- $\\theta_{B}^{(0)} = 0.50$\n",
    "\n",
    "The subscript denotes the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting the joint**\n",
    "\n",
    "The joint probability distribution of all the observables and the latent variables given the parameters is,\n",
    "\n",
    "$$ p\\left(X_{1}, X_{2}, .., X_{5}  \\mid  \\theta\\right) = p(X_{1}, X_{2}, .., X_{5}, z_{1}, z_{2}, .., z_{5}, \\theta) $$\n",
    "\n",
    "where $X_{i} = \\{x_{i}^{1}, ..., x_{i}^{10}\\}$ is result of the coin toss, $z_{i}$ is the latent variable (e.g. which coin is being used) and $\\theta$ is a set of parameters (e.g. probability of getting a heads and probability of getting a tails).\n",
    "\n",
    "We do not know what the latent variable is, therefore we need to take it out of the probability function using `Bayes' theorem`.\n",
    "\n",
    "$$ p\\left(X_{1}, X_{2}, .., X_{5}, z_{1}, z_{2}, .., z_{5}  \\mid  \\theta\\right) = p(X_{1}, X_{2}, .., X_{5} \\mid \\theta)p(z_{1}, z_{2}, .., z_{5}) $$\n",
    "\n",
    "The probabilities of the observables (groups of flips) are independent, therefore,\n",
    "\n",
    "$$ p\\left(X_{1}, X_{2}, .., X_{5}  \\mid  \\theta\\right) = \\prod_{i=1}^{5}p(\\{x_{i}^{1}, .., x_{i}^{10}\\} \\mid z_{i}, \\theta) \\prod_{i=1}^{5}p(z_{i}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the equation above into it's components.\n",
    "\n",
    "We can find the joint probability distribution of the group of flips by assuming they are also independent, therefore, \n",
    "\n",
    "$$ p(\\{x_{i}^1, .., x_{i}^{10}\\} \\mid z_{i}, \\theta) = \\prod_{j=1}^{10} p(x_{i}^{j} \\mid z_{i}, \\theta) $$\n",
    "\n",
    "Furthermore, we can find find the proability $p(x_{i}^{j} \\mid z_{i}, \\theta)$ by using the Bernoulli equation for a coin toss,\n",
    "\n",
    "$$ p(x_{i}^{j} \\mid z_{i}, \\theta) = \\prod_{k=1}^{2} \\left[ \\theta_{k}^{x_{i}^{j}} (1-\\theta_{k})^{1-x_{i}^{j}} \\right]^{z_{ik}} $$\n",
    "\n",
    "Also, we can find the probability of choosing either side of the coin (heads or tails),\n",
    "\n",
    "$$ p(z_{i}) = \\prod_{k=1}^{2} \\pi_{k}^{z_{ik}} $$\n",
    "\n",
    "where $\\pi_{k}$ is the probability of selecting coin $k \\in \\{1, 2\\}$\n",
    "\n",
    "The equations above are `selector functions` which will choose $k$ (heads or tails). The latent variable $z_{ik}$ is unknown but can either be 1 or 0 for either heads or tails, thereby `selecting` which coin it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can put all these equations together,\n",
    "\n",
    "$$ p\\left(X_{1}, X_{2}, .., X_{5}, z_{1}, z_{2}, .., z_{5}  \\mid  \\theta\\right) = \\prod_{i=1}^{5}\\prod_{j=1}^{10} \\prod_{k=1}^{2} \\left[ \\theta_{k}^{x_{i}^{j}} (1-\\theta_{k})^{1-x_{i}^{j}} \\right]^{z_{ik}} \\prod_{i=1}^{5} \\prod_{k=1}^{2} \\pi_{k}^{z_{ik}}.  $$\n",
    "\n",
    "Unfortunately, we don't know the latent variable ($z_{ik}$) and, therefore, we will need to use the expectation to remove replace it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing the expectation**\n",
    "\n",
    "We need to find the expectation so that we can replace the latent variable.\n",
    "\n",
    "First, take the log of probability,\n",
    "\n",
    "$$ \\log p\\left(X_{1}, X_{2}, .., X_{5}, z_{1}, z_{2}, .., z_{5}  \\mid  \\theta\\right) = \\sum_{i=1}^{5}\\sum_{j=1}^{10} \\sum_{k=1}^{2} z_{ik} \\log \\theta_{k}^{x_{i}^{j}} (1-\\theta_{k})^{1-x_{i}^{j}} + \\sum_{i=1}^{5} \\sum_{k=1}^{2} z_{ik} \\log \\pi_{k}.  $$\n",
    "\n",
    "Now take the expectation,\n",
    "\n",
    "$$ E_{p(Z \\mid X)}\\left[\\log p\\left(X_{1}, X_{2}, .., X_{5}, z_{1}, z_{2}, .., z_{5}  \\mid  \\theta\\right) \\right] = \\sum_{i=1}^{5}\\sum_{j=1}^{10} \\sum_{k=1}^{2} E_{p(Z \\mid X)}[z_{ik}] \\log \\theta_{k}^{x_{i}^{j}} (1-\\theta_{k})^{1-x_{i}^{j}} + \\sum_{i=1}^{5} \\sum_{k=1}^{2} E_{p(Z \\mid X)}[z_{ik}] \\log \\pi_{k}.  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expectation Step**\n",
    "\n",
    "This requires the calculation for the probability distribution of the latent variable given the observables ($X$) and the parameter ($\\theta$),\n",
    "\n",
    "$$ p(Z \\mid X, \\theta) = p(z_1, z_2, .., z_5 \\mid X_1, X_2, .., X_5, \\theta). $$\n",
    "\n",
    "Therefore, the expection is,\n",
    "\n",
    "$$ E_{p(Z \\mid X)}[z_{ik}] = \\sum_{z_{1}} ... \\sum_{z_{5}} z_{ik}p(Z \\mid X, \\theta) = \\sum_{z_i} z_{ik}p(z_i \\mid \\{x_i^{1}, .., x_i^{10}\\}, \\theta), $$\n",
    "\n",
    "where the probability can be calculated using `Bayes' theorem`,\n",
    "\n",
    "$$ p(z_i \\mid \\{x_i^{1}, .., x_i^{10}\\}, \\theta) = \\frac{p(\\{x_i^{1}, .., x_i^{10}\\} \\mid z_i, \\theta)p(z_i)}{p(\\{x_i^{1}, .., x_i^{10}\\} \\mid  \\theta)}. $$\n",
    "\n",
    "We can substitute in previous the previous equations that we found for $p(\\{x_i^{1}, .., x_i^{10}\\} \\mid z_i, \\theta)$ and $p(z_i)$.\n",
    "\n",
    "$$ p(z_i \\mid \\{x_i^{1}, .., x_i^{10}\\}, \\theta) = \\frac{\\prod_{j=1}^{10} \\prod_{k=1}^{2} \\left[ \\theta_{k}^{x_{i}^{j}} (1-\\theta_{k})^{1-x_{i}^{j}} \\right]^{z_{ik}} \\pi_{k}^{z_{ik}}}{\\sum_{z_i} \\prod_{j=1}^{10} \\prod_{k=1}^{2} \\left[ \\theta_{k}^{x_{i}^{j}} (1-\\theta_{k})^{1-x_{i}^{j}} \\right]^{z_{ik}} \\pi_{k}^{z_{ik}}}. $$\n",
    "\n",
    "The equation above can be used to find the expectation by substituting it in,\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "E_{p(Z \\mid X)}[z_{ik}] &= \\sum_{z_i} z_{il} \\frac{\\prod_{j=1}^{10} \\prod_{l=1}^{2} \\left[ \\theta_{l}^{x_{i}^{j}} (1-\\theta_{l})^{1-x_{i}^{j}} \\right]^{z_{il}} \\pi_{l}^{z_{il}}}{\\sum_{z_i} \\prod_{j=1}^{10} \\prod_{l=1}^{2} \\left[ \\theta_{l}^{x_{i}^{j}} (1-\\theta_{l})^{1-x_{i}^{j}} \\right]^{z_{il}} \\pi_{l}^{z_{il}}} \\\\\n",
    "&= \\frac{\\sum_{z_i} z_{il} \\prod_{j=1}^{10} \\prod_{l=1}^{2} \\left[ \\theta_{l}^{x_{i}^{j}} (1-\\theta_{l})^{1-x_{i}^{j}} \\right]^{z_{il}} \\pi_{l}^{z_{il}}}{\\sum_{z_i} \\prod_{j=1}^{10} \\prod_{l=1}^{2} \\left[ \\theta_{l}^{x_{i}^{j}} (1-\\theta_{l})^{1-x_{i}^{j}} \\right]^{z_{il}} \\pi_{l}^{z_{il}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can now simplify the expression using the fact that the $z_{i}$ acts as a selector and, therefore, is either 0 or 1 depending on $i$.\n",
    "\n",
    "$$ E_{p(Z \\mid X)}[z_{ik}] = \\frac{\\pi_{k} \\prod_{j=1}^{10}  \\theta_{k}^{x_{i}^{j}} (1-\\theta_{k})^{1-x_{i}^{j}}} {\\pi_{1} \\prod_{j=1}^{10}  \\theta_{1}^{x_{i}^{j}} (1-\\theta_{1})^{1-x_{i}^{j}} + \\pi_{2} \\prod_{j=1}^{10}  \\theta_{2}^{x_{i}^{j}} (1-\\theta_{2})^{1-x_{i}^{j}}}. $$\n",
    "\n",
    "Above is an equation for the expection of $z_{ik}$ and can be evaluated by fixing $\\theta$ and, thus, fixing the values for $\\pi_i$.\n",
    "\n",
    "The probability of picking up each coin, $p(z_i)$, is equal; therefore,:\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\pi_{1} = \\frac{1}{2}, && \\pi_{2} = \\frac{1}{2} \n",
    "\\end{align}\n",
    "$$.\n",
    "\n",
    "Therefore, the expectation evaluates to,\n",
    "\n",
    "$$ E_{p(Z \\mid X)}[z_{ik}] = \\frac{\\prod_{j=1}^{10}  \\theta_{k}^{x_{i}^{j}} (1-\\theta_{k})^{1-x_{i}^{j}}} { \\prod_{j=1}^{10}  \\theta_{1}^{x_{i}^{j}} (1-\\theta_{1})^{1-x_{i}^{j}} + \\prod_{j=1}^{10}  \\theta_{2}^{x_{i}^{j}} (1-\\theta_{2})^{1-x_{i}^{j}}}. $$\n",
    "\n",
    "This shows that the expectation is the posterior,\n",
    "\n",
    "$$ E_{p(Z \\mid X)}[z_{ik}] = p(z_{ik} \\mid {x_i^1, .., x_i^{10}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximization Step**\n",
    "\n",
    "We now go back to the end of `Computing the expectation` where we found an equation for the expection of the log of the joint probability distribution. We aim to maximise this equation, thus finding the probability of each coin being heads. The value for $E_{p(Z \\mid X)}[z_{ik}]$ is assumed to be fixed.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "max \\ L(\\theta) &=  E_{p(Z \\mid X)}\\left[\\log p\\left(X_{1}, X_{2}, .., X_{5}, z_{1}, z_{2}, .., z_{5}  \\mid  \\theta\\right) \\right] \\\\\n",
    "&= \\sum_{i=1}^{5}\\sum_{j=1}^{10} \\sum_{k=1}^{2} E_{p(Z \\mid X)}[z_{ik}] \\log \\theta_{k}^{x_{i}^{j}} (1-\\theta_{k})^{1-x_{i}^{j}} + \\sum_{i=1}^{5} \\sum_{k=1}^{2} E_{p(Z \\mid X)}[z_{ik}] \\log \\pi_{k} \\\\\n",
    "&= \\sum_{i=1}^{5}\\sum_{j=1}^{10} \\sum_{k=1}^{2} E_{p(Z \\mid X)}[z_{ik}] \\left( x_i^j \\log \\theta_{k} + (1-x_i^j) \\log (1-\\theta_k) \\right) + const\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The rules of $\\log$ have been applied above and the const arises due to the value of $\\pi$ and $E_{p(Z \\mid X)}[z_{ik}]$ being constant. The equation now only has observations ($x$) and parameters ($\\theta$).\n",
    "\n",
    "We can find the maximum value for $\\theta$ by differentiating the equation above and setting it equal to 0. This can be done for $k=1$ and $k=2$ seperately.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\frac{dL(\\theta)}{d\\theta_1} &= \\sum_{i=1}^{5}\\sum_{j=1}^{10} E_{p(Z \\mid X)}[z_{i1}] \\left ( x_i^j \\frac{1}{\\theta_{1}} + (1-x_i^j) \\frac{1}{(1-\\theta_1)} \\right ) \\\\\n",
    "0 &= \\sum_{i=1}^{5}\\sum_{j=1}^{10} E_{p(Z \\mid X)}[z_{i1}] \\left ( x_i^j (1 -\\theta_{1}) + (1-x_i^j) \\theta_1 \\right )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Finally, we have our values for $\\theta_1$ and $\\theta_2$, the probabilities of the coins being heads,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\theta_1 = \\frac{\\sum_{i=1}^{5}\\sum_{j=1}^{10} E_{p(Z \\mid X)}[z_{i1}] x_i^j}{\\sum_{i=1}^{5} 10 \\ E_{p(Z \\mid X)}[z_{i1}]}, && \\theta_2 = \\frac{\\sum_{i=1}^{5}\\sum_{j=1}^{10} E_{p(Z \\mid X)}[z_{i2}] x_i^j}{\\sum_{i=1}^{5} 10 \\ E_{p(Z \\mid X)}[z_{i2}]}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method**\n",
    "\n",
    "Mixture models enable clustering of un-labelled data (unsupervised learning). They can detect a given number of clusters in a multi dimensional feature space given that each cluster is has the distribution of a Gaussian. Other mixture model can be used which have other underlying distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Priors**\n",
    "\n",
    "We think there are 3 clusters in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting the joint**\n",
    "\n",
    "We can formulate the joint probability by assuming that the $N$ observables are independent. Therefore, the probability of the observables $(X)$ and the latent variable $(Z)$ given the parameter $(\\theta),$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(X,Z \\mid \\theta) &= p(x_1, x_2, .., x_N, z_1, z_2, .., z_N \\mid \\theta) \\\\\n",
    "&= \\prod_{n=1}^N p(x_n \\mid z_n, \\theta_x) \\prod_{n=1}^N p(z_n \\mid \\theta_z)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 parameters describing the shape of each Guassian distribution,\n",
    "\n",
    "$$ \\theta_x = \\{\\Sigma_1, \\Sigma_2, \\Sigma_3, \\mu_1, \\mu_2, \\mu_3 \\}, $$\n",
    "\n",
    "and 3 describing the probabilities of choosing one of the three clusters,\n",
    "\n",
    "$$ \\theta_z = \\{ \\pi_1, \\pi_2, \\pi_3 \\}. $$\n",
    "\n",
    "We use the Guassian ($\\mathcal{N}$) to describe the probability of getting an observable given the latent variable and parameters,\n",
    "\n",
    "$$ p(x_n \\mid z_n, \\theta) = \\prod_{k=1}^3 \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)^{z_{nk}}, $$\n",
    "\n",
    "Again we have the probability of the latent variable, \n",
    "\n",
    "$$ p(z_n) = \\prod_{k=1}^3\\pi_k^{z_{nk}}, $$\n",
    "\n",
    "where are `selector functions` have 3 probabilities to choose from.\n",
    "\n",
    "Substituting these functions in gives,\n",
    "\n",
    "$$ p(X,Z \\mid \\theta) = \\prod_{n=1}^N \\prod_{k=1}^3 \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)^{z_{nk}} \\prod_{n=1}^N \\prod_{k=1}^3\\pi_k^{z_{nk}}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing the expectation**\n",
    "\n",
    "Now take the log of the probability,\n",
    "\n",
    "$$ \\log \\ p(X,Z \\mid \\theta) = \\sum_{n=1}^N \\sum_{k=1}^3 z_{nk} \\left \\{ \\log \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k) + \\log \\pi_k \\right \\}. $$\n",
    "\n",
    "And find the expectation of the latent variable $Z$ given the observable and parameters,\n",
    "\n",
    "$$ E_{p(Z \\mid X, \\theta)} [ \\log \\ p(X,Z \\mid \\theta)] = \\sum_{n=1}^N \\sum_{k=1}^3 E_{p(Z \\mid X, \\theta)}[z_{nk}] \\left \\{ \\log \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k) + \\log \\pi_k \\right \\}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expectation Step**\n",
    "\n",
    "$$ E_{p(Z \\mid X, \\theta)}[z_{nk}] = \\sum_{z_n} z_{nk} \\ p(z_n \\mid x_n, \\theta^{old} ) $$\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "p(z_n \\mid x_n, \\theta^{old}) &= \\frac{p(x_n, z_n \\mid \\theta^{old} )}{p(x_n \\mid \\theta^{old})} \\\\\n",
    "&= \\frac{p(x_n \\mid z_n, \\theta^{old} )  p(z_n \\mid \\theta^{old} )}{p(x_n \\mid \\theta^{old})} \\\\\n",
    "&= \\frac{\\prod_{k=1}^3 \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)^{z_{nk}} \\pi_k^{z_{nk}}}{\\sum_{z_{nk}} \\prod_{k=1}^3 \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)^{z_{nk}} \\pi_k^{z_{nk}}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximization step**\n",
    "\n",
    "We can construct $G(\\theta)$, our expectation of the log of the joint probability distribution.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G(\\theta) &= E_{p(Z \\mid X, \\theta)}  [\\log \\ p(X,Z \\mid \\theta)] \\\\\n",
    "&= \\sum_{n=1}^N \\sum_{k=1}^3 E_{p(Z \\mid X, \\theta)}[z_{nk}] \\left \\{ \\log \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k) + \\log \\pi_k \\right \\} \\\\\n",
    "&= \\sum_{n=1}^N \\sum_{k=1}^3 \\gamma(z_{nk}) \\left\\{ -\\frac{1}{2}(x_n - \\mu_k)^T \\Sigma_k^{-1} (x_n - \\mu_k) - \\frac{1}{2}(F \\log2 \\pi + \\log |\\Sigma_k|) + \\log \\pi_k \\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\gamma(z_{nk})$ (the posterior or responsibility) is the $E_{p(Z \\mid X, \\theta)}[z_{nk}]$ and the multivariant Gaussian distribution that was substituted in is,\n",
    "\n",
    "$$ \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k) = (2\\pi)^{-\\frac{F}{2}} |\\Sigma_k|^\\frac{1}{2} exp\\left(-\\frac{1}{2}(x_k-\\mu_k)^T \\Sigma^{-1}(x_k-\\mu_k) \\right), $$\n",
    "\n",
    "where $F$ is the number of dimensions of the observable ($X$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to differential $G(\\theta)$ by $\\mu_k$ and by $\\Sigma_k$ to find the value of these parameters which maximises the expectation, $G(\\theta)$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dG(\\theta)}{d\\mu_k} = 0, && \\frac{dG(\\theta)}{d\\Sigma_k} = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dG(\\theta)}{d\\mu_k} &= \\sum_{n=1}^N \\gamma(z_{nk}) \\Sigma_k^{-1} (x_n -\\mu_k) = 0 \\\\\n",
    "u_k &= \\frac{\\sum_{n=1}^N \\gamma(z_{nk}) x_n}{\\sum_{n=1}^N \\gamma(z_{nk})} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dG(\\theta)}{d\\Sigma_k} &= \\sum_{n=1}^N \\gamma(z_{nk}) \\left\\{ (x_n-\\mu_k)(x_n-\\mu_k)^T - \\Sigma_k \\right\\} = 0 \\\\\n",
    "\\Sigma_k &= \\frac{\\sum_{n=1}^N \\gamma(z_{nk}) (x_n-\\mu_k)(x_n-\\mu_k)^T}{\\sum_{n=1}^N \\gamma(z_{nk})} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now need to find the last parameter $\\pi_k$, we can use our knowledge that the sum of the probabilities must equal 1, therefore,\n",
    "\n",
    "$$ \\sum_{k=1}^3 \\pi_k = 1 $$\n",
    "\n",
    "By rearranging the equation above, we can create a new function called $L(\\theta)$ that we can minimise which incorporates the condition above,\n",
    "\n",
    "$$ L(\\theta) = G(\\theta) - \\lambda \\left ( \\sum_{k=1}^3 \\pi_k - 1 \\right). $$\n",
    "\n",
    "This new function is differentiated with respect to $\\pi_k$, and value of $\\pi_k$ that maximises the function can be found,\n",
    "\n",
    "$$ \\pi_k = \\frac{\\sum_{n=1}^{N} \\gamma(z_{nk})}{N}. $$\n",
    "\n",
    "All the parameters have now been found to find the probability that each point is in a given cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
